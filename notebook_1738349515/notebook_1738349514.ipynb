{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c993165a",
   "metadata": {},
   "source": [
    "# AI Generated Notebook\n",
    "Generated on: 2025-02-01 00:21:54\n",
    "\n",
    "## Prompt\n",
    "scaping a python blog website \n",
    "\n",
    "## Description\n",
    "This notebook was automatically generated using AI based on the provided prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def scrape_blog(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        blog_data = {\n",
    "            'title': None,\n",
    "            'author': None,\n",
    "            'date': None,\n",
    "            'tags': [],\n",
    "            'content': None,\n",
    "            'comments': []\n",
    "        }\n",
    "\n",
    "        title_element = soup.find('h1', class_=re.compile(r'title|entry-title|post-title', re.IGNORECASE))\n",
    "        if title_element:\n",
    "           blog_data['title'] = title_element.text.strip()\n",
    "\n",
    "        author_element = soup.find(class_=re.compile(r'author|byline', re.IGNORECASE))\n",
    "        if author_element:\n",
    "            blog_data['author'] = author_element.text.strip()\n",
    "        \n",
    "        date_element = soup.find(class_=re.compile(r'date|post-date|entry-date', re.IGNORECASE))\n",
    "        if date_element:\n",
    "           blog_data['date'] = date_element.text.strip()\n",
    "\n",
    "        tags_elements = soup.find_all(class_=re.compile(r'tag|tags|post-tags|entry-tags', re.IGNORECASE))\n",
    "        for tag_element in tags_elements:\n",
    "            tags = [tag.text.strip() for tag in tag_element.find_all('a')]\n",
    "            blog_data['tags'].extend(tags)\n",
    "        blog_data['tags'] = list(set(blog_data['tags']))\n",
    "\n",
    "        content_element = soup.find(class_=re.compile(r'content|entry-content|post-content', re.IGNORECASE))\n",
    "        if content_element:\n",
    "            paragraphs = content_element.find_all('p')\n",
    "            blog_data['content'] = '\\n'.join([p.text.strip() for p in paragraphs])\n",
    "\n",
    "        comment_elements = soup.find_all(class_=re.compile(r'comment|comment-body', re.IGNORECASE))\n",
    "        for comment_element in comment_elements:\n",
    "             comment_text = comment_element.text.strip()\n",
    "             if comment_text:\n",
    "                 blog_data['comments'].append(comment_text)\n",
    "\n",
    "\n",
    "        return blog_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    blog_url = \"https://realpython.com/python-web-scraping-practical-introduction/\"\n",
    "    scraped_data = scrape_blog(blog_url)\n",
    "\n",
    "    if scraped_data:\n",
    "        print(\"Scraped Blog Data:\")\n",
    "        for key, value in scraped_data.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}